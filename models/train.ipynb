{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "712421a0-3b6d-441a-8bf7-f617cbde1e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "def import_notebook(nb_path):\n",
    "    with open(nb_path, 'r') as f:\n",
    "        nb = nbformat.read(f, as_version=4)\n",
    "    shell = InteractiveShell.instance()\n",
    "    for cell in nb.cells:\n",
    "        if cell.cell_type == 'code':\n",
    "            shell.run_cell(cell.source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25bb22cc-311e-4825-b6a1-6c32041ae1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "import_notebook('model1.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca083bd7-e4b8-4c45-9eb6-4f6b8f930279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f793fc1e-4c8b-4b8f-aed1-66636e3a7f53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1248bbe5-36e0-4de6-a87b-97ad8d76c6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext.datasets as datasets\n",
    "from torch.utils.data import Dataset,DataLoader,random_split\n",
    "from torch.optim.lr_scheduler import LambdaLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "834f3b53-2db8-4cc7-85c6-fee961e3eafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e861346f-236b-43c7-a072-90d434b40246",
   "metadata": {},
   "outputs": [],
   "source": [
    "def casual_mask(size):\n",
    "    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n",
    "    return mask == 0\n",
    "class BilingualDataset(Dataset):\n",
    "    def __init__(self,ds,input_token,output_token,input_lang,output_lang,seq_lenght):\n",
    "        super().__init__()\n",
    "        self.seq_len=seq_lenght\n",
    "        self.raw_data=ds\n",
    "        self.input_token_list=input_token\n",
    "        self.input_lang=input_lang\n",
    "        self.output_token_list=output_token\n",
    "        self.output_lang=output_lang\n",
    "        self.sos_id=torch.tensor([output_token.token_to_id(\"[SOS]\")],dtype=torch.int64)\n",
    "        self.eos_id=torch.tensor([output_token.token_to_id(\"[EOS]\")],dtype=torch.int64)\n",
    "        self.pad_id=torch.tensor([output_token.token_to_id(\"[PAD]\")],dtype=torch.int64)\n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    "    def __getitem__(self,id):\n",
    "            \n",
    "        input_output=self.raw_data[id]\n",
    "\n",
    "            #dividing the text into input and output\n",
    "        input_text=input_output['translation'][self.input_lang]\n",
    "        output_text=input_output['translation'][self.output_lang]\n",
    "            ### converting text into list to token\n",
    "        input_sen_token=self.input_token_list.encode(input_text).ids\n",
    "        output_sen_token=self.output_token_list.encode(output_text).ids\n",
    "            ### adding padding[PAD] , endofdentense[EOS],startofsentense[SOS] to the sentnse to\n",
    "            ### to make it equal to seqence length\n",
    "        num_encod_padding=self.seq_len-len(input_sen_token)-2 #2==[SOS]&[EOS]\n",
    "        num_decod_padding=self.seq_len-len(output_sen_token)-1 #1==[SOS]\n",
    "        num_label_padding=self.seq_len-len(output_sen_token)-1 ##1==[EOS]\n",
    "        if  num_encod_padding< 0 or num_decod_padding < 0:\n",
    "            raise ValueError(\"Sentence is too long\")\n",
    "            ### forming the whole sentence of encoding and decoding input\n",
    "        encoding_input=torch.cat([self.sos_id,torch.tensor(input_sen_token,dtype=torch.int64),\n",
    "                                     self.eos_id,\n",
    "                                     torch.tensor([self.pad_id]*num_encod_padding,dtype=torch.int64)\n",
    "                                     ],dim=0)\n",
    "        decoding_input=torch.cat([self.sos_id,torch.tensor(output_sen_token,dtype=torch.int64),\n",
    "                                     \n",
    "                                     torch.tensor([self.pad_id]*num_decod_padding,dtype=torch.int64)\n",
    "                                     ],dim=0)\n",
    "        label_input=torch.cat([torch.tensor(output_sen_token,dtype=torch.int64),\n",
    "                                     self.eos_id,\n",
    "                                     torch.tensor([self.pad_id]*num_label_padding,dtype=torch.int64)\n",
    "                                     ],dim=0)\n",
    "        assert encoding_input.size(0)==self.seq_len\n",
    "        assert decoding_input.size(0)==self.seq_len\n",
    "        assert label_input.size(0)==self.seq_len\n",
    "        return {\n",
    "                \"encoding_input\":encoding_input,\n",
    "                \"decoding_input\":decoding_input,\n",
    "                \"encodig_mask\":(encoding_input!=self.pad_id).unsqueeze(0).unsqueeze(0).int(),     # (1, 1, seq_len)\n",
    "                \"decoder_mask\": (decoding_input != self.pad_id).unsqueeze(0).int() & casual_mask(decoding_input.size(0)),\n",
    "                \"label\": label_input,\n",
    "                \"encoding_text\":input_text,\n",
    "                \"decoding_text\":output_text\n",
    "            \n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f4d4c3b-fa6f-407f-9939-44dacc776d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config():\n",
    "    return {\n",
    "        \"batch_size\": 8,\n",
    "        \"num_epochs\": 3,\n",
    "        \"lr\": 10**-4,\n",
    "        \"seq_len\": 350,\n",
    "        \"d_model\": 512,\n",
    "        \"datasource\": 'findnitai',\n",
    "        \"input_lang\": \"en\",\n",
    "        \"output_lang\": \"hi_ng\",\n",
    "        \"model_folder\": \"weights\",\n",
    "        \"model_basename\": \"tmodel_\",\n",
    "        \"preload\": \"latest\",\n",
    "        \"tokenizer_file\": \"tokenizer_{0}.json\",\n",
    "        \"experiment_name\": \"runs/tmodel\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4da7425b-36c1-4402-b885-d2ba173f52fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = [\n",
    "#     {\"translation\": {\"en\": \"Hello world\", \"fr\": \"Bonjour le monde\"}},\n",
    "#     {\"translation\": {\"en\": \"How are you?\", \"fr\": \"Comment Ã§a va?\"}},\n",
    "#     {\"translation\": {\"en\": \"I am fine.\", \"fr\": \"Je vais bien.\"}},\n",
    "#     {\"translation\": {\"en\": \"Thank you.\", \"fr\": \"Merci.\"}},\n",
    "#     {\"translation\": {\"en\": \"Goodbye!\", \"fr\": \"Au revoir!\"}}\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ddb3c61-9118-4e06-946e-b5f5f1ba7d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_sentences(ds,lang):\n",
    "    for item in ds:\n",
    "        yield item['translation'][lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0237fba-52ca-440a-8c8c-0e5e7bd8fc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_build_tokenizer(ds,lang):\n",
    "    tokenizer_path=Path(f'token{lang}.json')\n",
    "    if not Path.exists(tokenizer_path):\n",
    "        tokenizer=Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = Whitespace()\n",
    "        trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds,lang),trainer=trainer)\n",
    "        tokenizer.save(str(tokenizer_path))\n",
    "    else:\n",
    "         tokenizer=Tokenizer.from_file(str(tokenizer_path))\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1885e590-f479-47ad-8d1c-e9f9acd1c40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ds(config):\n",
    "    #data_set_path form hugging face\n",
    "    raw_data =load_dataset(f\"findnitai/english-to-hinglish\",split='train')\n",
    "    ## Tokenization of data set\n",
    "    token_input=get_or_build_tokenizer(raw_data,config['input_lang'])\n",
    "    token_ouput=get_or_build_tokenizer(raw_data,config['output_lang'])\n",
    "    ## split the data set \n",
    "    print(f\"raw-----{len(raw_data)}\")\n",
    "    num_train_data=int(0.001*(len(raw_data)))\n",
    "    print(f\"train_data-----{(num_train_data)}\")\n",
    "    num_test_data=int(len(raw_data)-num_train_data)\n",
    "    train_raw_data,test_raw_data=random_split(raw_data,[num_train_data,num_test_data])\n",
    "    print(f\"train_raw_data{len(train_raw_data)}\")\n",
    "    ### converting each text sentense into token with equal size of seqence lenght each\n",
    "    train_ds=BilingualDataset(ds=train_raw_data,input_token=token_input,output_token=token_ouput,input_lang=config['input_lang'],output_lang=config['output_lang'],seq_lenght=config['seq_len'])\n",
    "    test_ds=BilingualDataset(ds=test_raw_data,input_token=token_input,output_token=token_ouput,input_lang=config['input_lang'],output_lang=config['output_lang'],seq_lenght=config['seq_len'])\n",
    "    ### dividing data set into batch\n",
    "    print(f\"train_ds{len(train_ds)}\")\n",
    "    train_ds_batch= DataLoader(train_ds,batch_size=config['batch_size'],shuffle=True)\n",
    "    test_ds_batch= DataLoader(test_ds,batch_size=1,shuffle=True)\n",
    "    print(f\"train_ds_batch--{len(train_ds_batch)}\")\n",
    "    return train_ds_batch,test_ds_batch,token_input,token_ouput\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8756fb31-9cbe-4d9a-bcef-1f042a59db8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###input_vocab_size: int, output_vocab_size: int, \n",
    "    #                   input_seq_len: int, output_seq_len: int, d_model: int=512,\n",
    "    # N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048\n",
    "def get_model(config,vocab_input_len,vocab_output_len):\n",
    "    model=build_transformer(input_vocab_size=vocab_input_len,output_vocab_size=vocab_output_len,\n",
    "                           input_seq_len=config['seq_len'],output_seq_len=config['seq_len']\n",
    "                           \n",
    "                           )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d54c3ab-a374-4d61-bfc7-7a6df603c256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n",
    "    device = torch.device(device)\n",
    "    train_data,test_data,token_input,token_output=get_ds(config)\n",
    "    print(len(train_data))\n",
    "    model=get_model(config,token_input.get_vocab_size(),token_output.get_vocab_size())\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr=0.01,eps=1e-9)\n",
    "    loss_fn=nn.CrossEntropyLoss(ignore_index=token_input.token_to_id('[PAD]'),label_smoothing=0.1).to(device)\n",
    "    for epoch in range(1,config['num_epochs']):\n",
    "        model.train()\n",
    "        batch_iterator = tqdm(train_data, desc=f\"Processing Epoch {epoch:02d}\")\n",
    "        for batch in batch_iterator:\n",
    "            encoder_input=batch['encoding_input'].to(device)\n",
    "            decoder_input = batch['decoding_input'].to(device)\n",
    "            encoder_mask = batch['encodig_mask'].to(device) # (B, 1, 1, seq_len)\n",
    "            decoder_mask = batch['decoder_mask'].to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            encoder_output=model.encode(encoder_input,encoder_mask)\n",
    "            decoder_output=model.decode(encoder_output, decoder_input,encoder_mask, decoder_mask)\n",
    "            proj_output=model.project(decoder_output)\n",
    "            \n",
    "            label=batch['label'].to(device)\n",
    "            loss=loss_fn(proj_output.view(-1,token_output.get_vocab_size()),label.view(-1))\n",
    "            batch_iterator.set_postfix({\"loss\": f\"{loss.item():6.3f}\"})\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "223441c8-3bb8-4b51-9a50-1c9750e41dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw-----189102\n",
      "train_data-----189\n",
      "train_raw_data189\n",
      "train_ds189\n",
      "train_ds_batch--24\n",
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Epoch 01: 100%|âââââââââââââââââââââââââââââââââââââââââââââââââ| 24/24 [03:36<00:00,  9.01s/it, loss=8.467]\n",
      "Processing Epoch 02: 100%|âââââââââââââââââââââââââââââââââââââââââââââââââ| 24/24 [03:19<00:00,  8.33s/it, loss=7.513]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    config = get_config()\n",
    "    train_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4d01ab-2728-4609-b8f3-aff2a478e3b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e5b3a8-e864-4af0-9fd2-7d415a75c438",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
