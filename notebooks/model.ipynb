{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "582b9ae7-0dc5-4e57-9a9e-a870530a3477",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3aa1d147-8d09-4772-9d21-b5560c65e92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0719,  0.2177,  0.0672,  0.2151],\n",
      "         [-0.0734,  0.1958,  0.0551,  0.1925],\n",
      "         [-0.0750,  0.1999,  0.0574,  0.1947]],\n",
      "\n",
      "        [[-0.0797,  0.2668,  0.0557,  0.3180],\n",
      "         [-0.0613,  0.2616,  0.0731,  0.3019],\n",
      "         [-0.0806,  0.2661,  0.0542,  0.3182]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# import math\n",
    "# from torch import nn\n",
    "\n",
    "# def test_multihead_attention_block():\n",
    "#     batch_size = 2\n",
    "#     seq_length = 3\n",
    "#     d_model = 4\n",
    "#     head = 2\n",
    "#     dropout = 0.1\n",
    "\n",
    "#     # Create a MultiheadAttentionBlock\n",
    "#     attention_block = MultiheadAttentionBlock(d_model, head, seq_length, dropout)\n",
    "\n",
    "#     # Create dummy inputs\n",
    "#     q = torch.rand(batch_size, seq_length, d_model)\n",
    "#     k = torch.rand(batch_size, seq_length, d_model)\n",
    "#     v = torch.rand(batch_size, seq_length, d_model)\n",
    "#     mask = torch.ones(batch_size, seq_length, seq_length)\n",
    "\n",
    "#     # Forward pass\n",
    "#     output = attention_block(q, k, v, mask)\n",
    "#     print(output)\n",
    "\n",
    "#     # Assertions\n",
    "#     assert output.shape == (batch_size, seq_length, d_model), \\\n",
    "#         f\"Expected output shape {(batch_size, seq_length, d_model)}, but got {output.shape}\"\n",
    "\n",
    "#     # Check that attention scores are calculated and have the right shape\n",
    "#     assert attention_block.attention_score.shape == (batch_size, head, seq_length, seq_length), \\\n",
    "#         f\"Expected attention scores shape {(batch_size, head, seq_length, seq_length)}, but got {attention_block.attention_score.shape}\"\n",
    "\n",
    "# # Run the test function\n",
    "# test_multihead_attention_block()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a6010541-2377-48c2-8e34-97a84e1b00af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttentionBlock(nn.Module):\n",
    "    def __init__(self,d_model,head,seq_length,dropout):\n",
    "        super().__init__()\n",
    "        assert d_model%head==0,f\"choose other head value as d_model-({d_model}) not completly divisible by number of head-({head})\"\n",
    "        self.d_model=d_model\n",
    "        self.h=head\n",
    "        self.d_k=int(self.d_model/self.h)\n",
    "        self.seq_length=seq_length\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        # sructure of the input (no.batch, seq_lenght, d_model)\n",
    "        self.weight_quarry=nn.Linear(d_model,d_model,bias=False)\n",
    "        self.weight_key=nn.Linear(d_model,d_model,bias=False)\n",
    "        self.weight_value=nn.Linear(d_model,d_model,bias=False)\n",
    "        self.weight_output=nn.Linear(d_model,d_model,bias=False)\n",
    "    @staticmethod\n",
    "    def attention (quarry,key,value,mask,dropout:nn.Dropout):\n",
    "        dk=quarry.shape[-1] # d_model\n",
    "        attention_score=(quarry@key.transpose(-2,-1))/math.sqrt(dk)\n",
    "        if mask is not None:\n",
    "            attention_score=attention_score.masked_fill_(mask==0,-1e9)\n",
    "        attention_score=attention_score.softmax(dim=-1)\n",
    "        if dropout is not None:\n",
    "            attention_score=dropout(attention_score)\n",
    "        return (attention_score@value), attention_score\n",
    "    def forward(self,q,k,v,mask):\n",
    "        q=self.weight_quarry(q)\n",
    "        k=self.weight_key(k)\n",
    "        v=self.weight_value(v)\n",
    "            #batch,seq_lenght,d_model --to-- batch,seq_lenght,head,d_k --- to -- batch,head,seq_length,d_k\n",
    "        q=q.view(q.shape[0],q.shape[1],self.h,self.d_k).transpose(1,2)\n",
    "        k=k.view(k.shape[0],k.shape[1],self.h,self.d_k).transpose(1,2)\n",
    "        v=v.view(v.shape[0],v.shape[1],self.h,self.d_k).transpose(1,2)\n",
    "            \n",
    "        x,self.attention_score=MultiheadAttentionBlock.attention(q,k,v,mask,self.dropout)\n",
    "        x=x.transpose(1,2).contiguous().view(x.shape[0],-1,self.h*self.d_k)\n",
    "        return self.weight_output(x)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7152291-3b77-4ceb-9f8c-060fe5703a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len) --> (batch, seq_len, d_model)\n",
    "        # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)\n",
    "    \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04258683-008d-44b8-bd1a-6545479cc5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,d_model:int,seq_length:int,dropout:float):\n",
    "        super().__init__()\n",
    "        self.d_model=d_model\n",
    "        self.seq_length=seq_length\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        posemb=torch.zeros(self.seq_length,self.d_model)\n",
    "        pos=torch.arange(0,seq_length,dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        posemb[:,0::2]=torch.sin(pos*div_term)\n",
    "        posemb[:,1::2]=torch.cos(pos*div_term)\n",
    "        pos = pos.unsqueeze(0) # (1, seq_len, d_model)\n",
    "        # Register the positional encoding as a buffer\n",
    "        self.register_buffer('pos', pos)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + (self.pos[:, :x.shape[1], :]).requires_grad_(False) # (batch, seq_len, d_model)\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d853bb47-51da-4124-aa36-da35d9605bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self,feature:int,eps:float=10**-6):\n",
    "        super().__init__()\n",
    "        self.feature=feature\n",
    "        # self.dropout=nn.Dropout\n",
    "        self.eps=eps\n",
    "        self.alfa=nn.Parameter(torch.ones(self.feature))\n",
    "        self.beta=nn.Parameter(torch.zeros(self.feature))\n",
    "    def forward(self,x):\n",
    "        mean=x.mean(dim=-1,keepdim=True)\n",
    "        std=x.std(dim=-1,keepdim=True)\n",
    "        return self.alfa*(x-mean)/(std+self.eps)+self.beta\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2f65501-c64e-4705-a5f0-da651b4307e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08fe2f38-c5ef-4108-b8c1-4581e62a1053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    \n",
    "        def __init__(self, features: int, dropout: float) -> None:\n",
    "            super().__init__()\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            self.norm = LayerNormalization(features)\n",
    "    \n",
    "        def forward(self, x, sublayer):\n",
    "            return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8be3d31e-f3b5-4aa5-b8a6-5bc36d20f9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,self_attention_block:MultiheadAttentionBlock,self_feed_forward_block:FeedForwardBlock,feature:int,dropout:float):\n",
    "        super().__init__()\n",
    "        self.multi_attention=self_attention_block\n",
    "        self.feed_forward=self_feed_forward_block\n",
    "        self.residual_block=nn.ModuleList([ResidualConnection(feature,dropout) for _ in range(2)])\n",
    "    def forward(self,x,mask):\n",
    "        x=self.residual_block[0](x,lambda x:self.multi_attention(x,x,x ,mask))\n",
    "        x=self.residual_block[1](x,self.feed_forward)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98f9862f-f584-4523-b9f7-7e04deae2978",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,feature: int,layers:nn.ModuleList):\n",
    "        super().__init__()\n",
    "        self.norm=LayerNormalization(feature)\n",
    "        self.layers=layers\n",
    "    def forward(self,x,mask):\n",
    "        for layer in self.layers:\n",
    "            x=layer(x,mask)\n",
    "            x=self.norm(x)\n",
    "        return x\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09426679-5f20-4556-87b6-0fa840454434",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,feature,multi_head_attention:MultiheadAttentionBlock,cross_head_attention:MultiheadAttentionBlock,feed_forward:FeedForwardBlock,dropout:float):\n",
    "        super().__init__()\n",
    "        self.multiheadattention=multi_head_attention\n",
    "        self.crossheadattention=cross_head_attention\n",
    "        self.feedforward=feed_forward\n",
    "        self.residualblock=nn.ModuleList([ResidualConnection(feature,dropout) for _ in range(3)])\n",
    "    def forward(self,decoder_x,encoder_output,encoder_mask,decoder_mask):\n",
    "        decoder_x=self.residualblock[0](decoder_x,lambda x:self.multiheadattention(decoder_x,decoder_x,decoder_x,decoder_mask))\n",
    "        \n",
    "        decoder_x=self.residualblock[1](decoder_x,lambda x:self.crossheadattention(decoder_x,decoder_x,encoder_output,encoder_mask))\n",
    "        decoder_x=self.residualblock[2](decoder_x,self.feedforward)\n",
    "        return decoder_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0914b1f8-b7bc-43d5-a868-92702600df67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization(features)\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0694aa49-1afd-4d36-87c6-17a8ccc619b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x) -> None:\n",
    "\n",
    "        return self.proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "be505dac-b562-449d-9b88-8082eb48ecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,encoder:Encoder,decoder:Decoder,projection: ProjectionLayer,\n",
    "                enco_embed:InputEmbeddings,deco_embed:InputEmbeddings,enco_postion:PositionalEncoding,\n",
    "                 deco_prosition:PositionalEncoding\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.encod=encoder\n",
    "        self.decod=decoder\n",
    "        self.enco_embed=enco_embed\n",
    "        self.deco_embed=deco_embed\n",
    "        self.enco_position=enco_postion\n",
    "        self.deco_position=deco_prosition\n",
    "        self.proj=projection\n",
    "    def encode(self,src,mask):\n",
    "        src=self.enco_embed(src)\n",
    "        src=self.enco_position(src)\n",
    "        src=self.encod(src,mask)\n",
    "        return src\n",
    "    def decode(self,encoder_output:torch.Tensor,tar:torch.Tensor,src_mask:torch.Tensor,trg_mask:torch.Tensor):\n",
    "        tar=self.deco_embed(tar)\n",
    "        tar=self.deco_position(tar)\n",
    "        return self.decod(tar,encoder_output,src_mask,trg_mask)\n",
    "    def project(self,x):\n",
    "        return self.proj(x)\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        # Encode the source sequence\n",
    "        encoder_output = self.encode(src, src_mask)\n",
    "        \n",
    "        # Decode the target sequence\n",
    "        decoder_output = self.decode(encoder_output, tgt, src_mask,tgt_mask)\n",
    "        \n",
    "        # Project to the output vocabulary size\n",
    "        output = self.project(decoder_output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "50611e6a-5229-4c80-9b6e-056393cfa410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transformer(input_vocab_size: int, output_vocab_size: int, \n",
    "                      input_seq_len: int, output_seq_len: int, d_model: int=512,\n",
    "    N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n",
    "    # for InputEmbedding parameter ---> d_model,input_vocab_size and in forward \"x\"\n",
    "    input_embedding=InputEmbeddings(d_model,vocab_size=input_vocab_size) \n",
    "    output_embedding=InputEmbeddings(d_model,vocab_size=output_vocab_size)\n",
    "    # for PositionalEncoding parameter ---> d_model,output_vocab_size, dropout and in forward \"x\"\n",
    "    input_pos_encoding=PositionalEncoding(d_model,input_seq_len,dropout=dropout)\n",
    "    output_pos_encoding=PositionalEncoding(d_model,output_seq_len,dropout=dropout)\n",
    "    \n",
    "    # list of encoding block that can be run N times \n",
    "    \n",
    "    encoding_blocks=[]\n",
    "    for _ in range(N):\n",
    "        # for MulMultiheadAttentionBlock parameter --> d_model,head,seq_length,dropout\n",
    "        # for forward parameter are q,k,v,mask\n",
    "        encoding_self_attention=MultiheadAttentionBlock(d_model,head=h,seq_length=input_seq_len,dropout=dropout)\n",
    "        # # for FeedFeedForwardBlock parameter --> d_model: int, d_ff: int, dropout: float\n",
    "        # for forward parameter are x\n",
    "        encoding_feed_forward=FeedForwardBlock(d_model,d_ff,dropout)\n",
    "        # for EncoderBloEncoderBlock parameter ---> self_attention_block:MultiheadAttentionBlock,\n",
    "        #self_feed_forward_block:FeedForwardBlock,feature:int,dropout:float\n",
    "        # for forward parameter is  x,mask   \n",
    "        encoder_block=EncoderBlock(self_attention_block=encoding_self_attention,self_feed_forward_block=\n",
    "                                encoding_feed_forward,feature=d_model,dropout=dropout,  \n",
    "                                  )\n",
    "        encoding_blocks.append(encoder_block)\n",
    "    decoding_blocks=[]\n",
    "    for _ in range(N):\n",
    "\n",
    "        decoding_self_attention=MultiheadAttentionBlock(d_model,head=h,seq_length=output_seq_len,dropout=dropout)\n",
    "        decoding_cross_attention=encoding_self_attention=MultiheadAttentionBlock(d_model,head=h,seq_length=output_seq_len,dropout=dropout)\n",
    "        decoding_feed_forward=FeedForwardBlock(d_model,d_ff,dropout)\n",
    "        # for DecoderBlock parameter are :--> self,feature,multi_head_attention:MultiheadAttentionBlock,cross_head_attention:MultiheadAttentionBlock,\n",
    "        # feed_forward:FeedForwardBlock,dropout:float\n",
    "        decoder_block=DecoderBlock(feature=d_model,multi_head_attention=decoding_self_attention,\n",
    "                                  cross_head_attention=decoding_cross_attention,feed_forward=decoding_feed_forward\n",
    "                                  ,dropout=dropout)\n",
    "        decoding_blocks.append(decoder_block)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #unsing using the Encoder and Decoder to merger the EncoderBlock and DecoderBlock\n",
    "    ## parameter for Encoder :-->layers:nn.ModuleList,norm: LayerNormalization\n",
    "    ## for forward :-->\"x \" & \"mask\"\n",
    "    encoder=Encoder(feature=d_model,layers=nn.ModuleList(encoding_blocks))\n",
    "    decoder=Decoder(features=d_model,layers=nn.ModuleList(decoding_blocks))\n",
    "    \n",
    "    # for ProProjectionLayer aprameter--> d_model, vocab_size and forward: --> x\n",
    "    projection=ProjectionLayer(d_model=d_model,vocab_size=output_vocab_size)\n",
    "\n",
    "\n",
    "###### create transformer ################\n",
    "    # **** parameter   ***************\n",
    "    ###encoder:Encoder,decoder:Decoder,projection: ProjectionLayer,\n",
    "     # enco_embed:InputEmbedding,deco_embed:InputEmbedding,enco_postion:ProjectionLayer,\n",
    "      # deco_prosition:ProjectionLayer\n",
    "    transformer=Transformer(encoder=encoder,decoder=decoder,projection=projection,enco_embed=\n",
    "                           input_embedding,deco_embed=output_embedding,enco_postion=input_pos_encoding,\n",
    "                            deco_prosition=output_pos_encoding )\n",
    "\n",
    "    \n",
    "    # Initialize the parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return transformer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "188bfa3d-8f22-40ec-87d3-63bf44d72ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_build_transformer():\n",
    "    input_vocab_size = 10000\n",
    "    output_vocab_size = 10000\n",
    "    input_seq_len = 10\n",
    "    output_seq_len = 10\n",
    "    d_model = 512\n",
    "    N = 6\n",
    "    h = 8\n",
    "    dropout = 0.1\n",
    "    d_ff = 2048\n",
    "\n",
    "    # Build the transformer model\n",
    "    transformer = build_transformer(\n",
    "        input_vocab_size=input_vocab_size,\n",
    "        output_vocab_size=output_vocab_size,\n",
    "        input_seq_len=input_seq_len,\n",
    "        output_seq_len=output_seq_len,\n",
    "        d_model=d_model,\n",
    "        N=N,\n",
    "        h=h,\n",
    "        dropout=dropout,\n",
    "        d_ff=d_ff\n",
    "    )\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    transformer.eval()\n",
    "\n",
    "    # Create dummy data\n",
    "    batch_size = 2\n",
    "    src = torch.randint(0, input_vocab_size, (batch_size, input_seq_len), dtype=torch.long)\n",
    "    tgt = torch.randint(0, output_vocab_size, (batch_size, output_seq_len), dtype=torch.long)\n",
    "    src_mask = torch.ones(batch_size, 1, input_seq_len, input_seq_len, dtype=torch.float)\n",
    "    tgt_mask = torch.ones(batch_size, 1, output_seq_len, output_seq_len, dtype=torch.float)\n",
    "\n",
    "    # Perform a forward pass through the model\n",
    "    output = transformer(src, tgt, src_mask, tgt_mask)\n",
    "\n",
    "    # Check if the output shape is as expected\n",
    "    expected_output_shape = (batch_size, output_seq_len, output_vocab_size)\n",
    "    assert output.shape == expected_output_shape, f\"Expected output shape {expected_output_shape}, but got {output.shape}\"\n",
    "\n",
    "    print(\"Test passed!\")\n",
    "\n",
    "# Run the test case\n",
    "test_build_transformer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a26e707-673d-485d-a0ce-b1a7247c68a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9d2087-5b2a-4720-bb2f-cbbc6fd4b730",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a20e9d0-8a02-4630-8284-83ac99dc075e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
